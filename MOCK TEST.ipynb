{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d203d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1)\n",
    "a=2000\n",
    "b=3200\n",
    "for i in range(a,b+1):\n",
    "    if i%5!=0 and i%7==0:\n",
    "        print(i,end=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)\n",
    "a=\"Hey\"\n",
    "def upper_lower(sentence):\n",
    "    u = 0\n",
    "    l = 0\n",
    "    for char in a:\n",
    "        if char.isupper():\n",
    "            u += 1\n",
    "        elif char.islower():\n",
    "            l += 1\n",
    "        return (u, l)\n",
    "\n",
    "result = upper_lower(a)\n",
    "print(f\"The number of upper case letters is {result[0]}.\")\n",
    "print(f\"The number of lower case letters is {result[1]}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ebc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3)\n",
    "1)Feature selection.\n",
    "2)We use a Principal Component Analysis (PCA) Apply PCA to transform the original features into \n",
    "a smaller set of uncorrelated features while retaining as much variance as possible. This can significantly reduce the number of features.\n",
    "It is used for the reduce the dimensitionality of the model.\n",
    "Additionally, it's essential to communicate with your manager and stakeholders to understand the trade-offs \n",
    "between reduced dimensionality and potential loss of information.\n",
    "Selecting the appropriate technique depends on the nature of your data and the requirements of your classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9760ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4)\n",
    "Model Performance:\n",
    "An accuracy of 96% in a cancer detection model is generally positive.\n",
    "but the Consider consequences of false positives/negatives.\n",
    "Assess metrics like sensitivity, specificity, precision, recall, and AUC-ROC.\n",
    "Validate with cross-validation to ensure robustness.\n",
    "Evaluate using a confusion matrix, ROC curves, and precision-recall curves.\n",
    "Naive Bayes Algorithm:\n",
    "In Naive Bayes:\n",
    "Prior Probability (Prior): Initial belief about class probability.\n",
    "Likelihood: Probability of features given a class, assuming feature independence.\n",
    "Marginal Likelihood (Evidence): Probability of observing features across all classes.\n",
    "Bayes' theorem combines these to calculate posterior probability and predict the most probable class. \n",
    "The \"naive\" assumption simplifies calculations, often effective in high-dimensional data like medical diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf21a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5)\n",
    "Yes, this can happen. A time series regression model can have higher accuracy than a decision tree model for several reasons:\n",
    "\n",
    "- A time series regression model can capture the temporal dependencies and trends in the data, while a decision tree model treats each observation as independent and ignores the time dimension.\n",
    "- A time series regression model can handle outliers and noise better than a decision tree model, which can be sensitive to minor changes in the data and prone to overfitting or underfitting.\n",
    "- A time series regression model can have a simpler and more interpretable structure than a decision tree model, which can become complex and unwieldy when there are many features and splits.\n",
    "\n",
    "However, this does not mean that a time series regression model is always superior to a decision tree model. The performance of any model depends on the characteristics and complexity of the data, the assumptions and constraints of the model, and the evaluation and validation methods used. Therefore, it is important to compare different models using appropriate criteria and metrics, and to tune the model parameters and features to optimize the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6)\n",
    "Regularization:Regularization can help to prevent overfitting and improve the generalization ability of the model. Some common regularization techniques are L1 (Lasso), L2 (Ridge).\n",
    "Ensemble methods: This involves combining multiple weak learners (such as decision trees) into a strong learner, which can reduce the variance and improve the accuracy of the model. Ensemble methods can also help to reduce the effect of noise and outliers in the data. Some common ensemble methods are Bagging, Boosting, and Stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7)\n",
    "No, I cann't remove correlated variables before doing PCA.\n",
    "PCA is a technique that can handle highly correlated variables by transforming them into a lower-dimensional space.\n",
    "Removing correlated variables before PCA may result in losing some information or introducing some bias in the analysis. \n",
    "Moreover, PCA can help to identify which variables are redundant or irrelevant, and thus can be removed or combined after the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8)\n",
    "Multicollinearity in regression occurs when two or more predictor variables are highly correlated with each other, such that they do not provide unique or independent information in the regression model. Multicollinearity can cause problems with the coefficient estimates, the p-values, and the model interpretation.\n",
    "\n",
    "\n",
    "If you find that your model has multicollinearity, you can still build a better model without losing any information by using one of the following methods:\n",
    "1)Regularization: This involves adding a penalty term to the cost function of your model, which reduces the complexity and magnitude of the model parameters.Regularization can help to prevent overfitting and improve the generalization ability of the model. Some common regularization techniques are L1 (Lasso), L2 (Ridge).\n",
    "2)Orthogonalization: This involves transforming the predictor variables into a set of new variables that are orthogonal (uncorrelated) to each other. Orthogonalization can help to eliminate the multicollinearity problem and simplify the model interpretation. Some common orthogonalization techniques are principal component analysis (PCA), singular value decomposition (SVD), and Gram-Schmidt process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9)\n",
    "Ridge regression and Lasso regression are both techniques for regularizing linear regression models and preventing overfitting. They both add a penalty term to the cost function, but with different approaches. Ridge regression shrinks the coefficients towards zero, while Lasso regression encourages some of them to be exactly zero.\n",
    "\n",
    "Ridge regression is favorable over Lasso regression when:\n",
    "\n",
    "The number of predictor variables is large and multicollinearity is present.\n",
    "The predictor variables have similar scales and importance.\n",
    "\n",
    "Lasso regression is favorable over Ridge regression when:\n",
    "\n",
    "The number of predictor variables is small or only a few of them are relevant.\n",
    "The predictor variables have different scales and importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ea957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10)\n",
    "Hello, this is Bing. I can help you with some information about tree splitting in binary classification.\n",
    "\n",
    "Tree splitting is the process of dividing the data into smaller and more homogeneous groups based on a set of rules or criteria. The goal is to create a tree structure that can accurately classify the data into two classes.\n",
    "\n",
    "The tree splitting starts from the root node, which contains the entire data set. The algorithm chooses a variable and a split point that can best separate the data into two child nodes, according to a certain measure of impurity or information gain. The impurity measures how mixed the classes are in a node, and the information gain measures how much the split reduces the impurity. Some common impurity measures are Gini index, entropy, and misclassification error.\n",
    "\n",
    "The algorithm then repeats the same process for each child node, until a stopping criterion is met. The stopping criterion can be based on the depth of the tree, the number of observations in a node, the purity of a node, or the improvement of the split.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e126f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11)\n",
    "Hello, this is Bing. I can help you with some information about OLS and other techniques for regression.\n",
    "\n",
    "OLS (ordinary least squares) is a method for estimating the coefficients of a linear regression model by minimizing the sum of the squared errors between the observed and predicted values. OLS is a bad option to work with when:\n",
    "\n",
    "The number of variables (p) is large compared to the number of observations (n). This can cause overfitting, multicollinearity, and high variance in the estimates.\n",
    "The variables are not independent and identically distributed (i.i.d.). This can violate the assumptions of OLS and lead to biased and inconsistent estimates.\n",
    "The relationship between the variables and the response is not linear. This can reduce the accuracy and interpretability of the model.\n",
    "\n",
    "Some techniques that are better than OLS in these situations are:\n",
    "\n",
    "-Regularization methods: These methods add a penalty term to the cost function of OLS, which reduces the complexity and magnitude of the coefficients. Regularization methods can help to prevent overfitting, reduce multicollinearity, and improve the generalization ability of the model. Some common regularization methods are L1 (Lasso), L2 (Ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12)\n",
    "def print_dict(number):\n",
    "    if 1 <= number <= 20:\n",
    "        d = {i: i**2 for i in range(1, number + 1)}\n",
    "        print(d)\n",
    "    else:\n",
    "        print(\"The number must be between 1 and 20\")\n",
    "\n",
    "# Test the function with some examples\n",
    "print_dict(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13)\n",
    "t = (1,2,3,4,5,6,7,8,9,10)\n",
    "t1 = t[:5]\n",
    "t2 = t[5:]\n",
    "print(*t1)\n",
    "print(*t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14)\n",
    "import random\n",
    "even_numbers = [i for i in range(100, 201) if i % 2 == 0]\n",
    "random_list = random.sample(even_numbers, 5)\n",
    "print(random_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
